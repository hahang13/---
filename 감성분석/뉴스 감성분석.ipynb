{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 감성분석 모델 불러오기\n",
    "# AutoTokenizer -> Tokenizer로 입력된 데이터를 token으로 바꾸는 역할\n",
    "# AutoModelForSequenceClassification은 Hugging Face 라이브러리의 Transformers에서 제공하는 클래스이며\n",
    "# 자동으로 시퀀스 분류 작업에 사용할 수 있는 사전 훈련된 모델을 선택하고, 이를 미세 조정(fine-tuning)하는 데 사용한다.\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from transformers import pipeline\n",
    "import pandas as pd\n",
    "import torch\n",
    "import tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xformers is not installed correctly. If you want to use memory_efficient_attention to accelerate training use the following command to install Xformers\n",
      "pip install xformers.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('snunlp/KR-FinBert-SC')\n",
    "model = AutoModelForSequenceClassification.from_pretrained('snunlp/KR-FinBert-SC')\n",
    "classifier = pipeline(task='text-classification', model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>title</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023.06.26. 오후 1:41</td>\n",
       "      <td>\\n\\t\\t\\t‘셀트리온 퀸즈 마스터즈’ 대회 설해원에서 성료...우박과 폭우속 재...</td>\n",
       "      <td>[\\n \\n설해원에서 열린 2023 셀트리온 퀸즈 마스터터즈 대회 장면. 사진제공 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2023-06-29</td>\n",
       "      <td>서정진 셀트리온 회장 복귀 3개월 줄줄이 부정 이슈...“어쩌나 주가!”</td>\n",
       "      <td>[\\n\\n\\n\\n\\n[마이데일리 = 구현주 기자] 서정진 셀트리온 회장이 경영복귀 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2023-06-29</td>\n",
       "      <td>내 주식은? 새로 지정된 코스닥 우량주 TOP 50</td>\n",
       "      <td>[\\n[방현철 박사의 머니머니] 실적, 지배구조, 건전성 우수한 코스닥 상장사는?\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2023-06-29</td>\n",
       "      <td>`24조 휴미라` 바이오시밀러, 사보험 길목 잡아라...삼바·셀트 `사활`</td>\n",
       "      <td>[\\n\\t\\t\\t삼성바이오에피스, 셀트리온이 24조원에 달하는 자가면역질환 치료제 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2023-06-29</td>\n",
       "      <td>[마켓뷰] 코스피, 기관 매도에 0.55% 하락…3일 연속 내리막</td>\n",
       "      <td>[\\n삼성전자, 장중 52주 신고가 썼지만 0.41% 하락코스닥지수도 0.6% 내린...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>622</th>\n",
       "      <td>2023-03-03</td>\n",
       "      <td>셀트리온 서정진 명예회장, 소방수로 경영 복귀</td>\n",
       "      <td>[\\n홀딩스 등 4사 사내이사 겸 이사회 공동의장 후보로회사측 “글로벌 불확실성 가...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>623</th>\n",
       "      <td>2023-03-03</td>\n",
       "      <td>셀트리온 서정진 컴백…바이오시밀러 美진출 총력</td>\n",
       "      <td>[\\n2년 임기 각 회사 사내이사 겸 공동의장 선임…28일 주총서 확정 명예회장 특...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>624</th>\n",
       "      <td>2023-03-03</td>\n",
       "      <td>서정진 셀트리온그룹 명예회장 경영 복귀…\"위기 극복\"</td>\n",
       "      <td>[\\n기사내용 요약경영 복귀안 이사회 의결…현 경영진 요청글로벌 경제 불확실성 가속...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>625</th>\n",
       "      <td>2023-03-03</td>\n",
       "      <td>서정진 셀트리온그룹 명예회장, 2년만에 경영 일선 복귀</td>\n",
       "      <td>[\\n이사회서 경영 복귀안 의결, 28일 주주총회 거쳐 확정글로벌 경제 불확실성 속...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>626</th>\n",
       "      <td>2023-03-02</td>\n",
       "      <td>‘신약 기지’ 완공 앞둔 셀트리온…“송도 연구센터 밸리데이션 돌입”</td>\n",
       "      <td>[\\nADC·mRNA 등 차세대 신약 플랫폼 개발제3공장 완공되면 생산능력 25만L...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>627 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    date                                              title   \n",
       "0    2023.06.26. 오후 1:41  \\n\\t\\t\\t‘셀트리온 퀸즈 마스터즈’ 대회 설해원에서 성료...우박과 폭우속 재...  \\\n",
       "1             2023-06-29           서정진 셀트리온 회장 복귀 3개월 줄줄이 부정 이슈...“어쩌나 주가!”   \n",
       "2             2023-06-29                       내 주식은? 새로 지정된 코스닥 우량주 TOP 50   \n",
       "3             2023-06-29          `24조 휴미라` 바이오시밀러, 사보험 길목 잡아라...삼바·셀트 `사활`   \n",
       "4             2023-06-29               [마켓뷰] 코스피, 기관 매도에 0.55% 하락…3일 연속 내리막   \n",
       "..                   ...                                                ...   \n",
       "622           2023-03-03                          셀트리온 서정진 명예회장, 소방수로 경영 복귀   \n",
       "623           2023-03-03                          셀트리온 서정진 컴백…바이오시밀러 美진출 총력   \n",
       "624           2023-03-03                      서정진 셀트리온그룹 명예회장 경영 복귀…\"위기 극복\"   \n",
       "625           2023-03-03                     서정진 셀트리온그룹 명예회장, 2년만에 경영 일선 복귀   \n",
       "626           2023-03-02              ‘신약 기지’ 완공 앞둔 셀트리온…“송도 연구센터 밸리데이션 돌입”   \n",
       "\n",
       "                                               content  \n",
       "0    [\\n \\n설해원에서 열린 2023 셀트리온 퀸즈 마스터터즈 대회 장면. 사진제공 ...  \n",
       "1    [\\n\\n\\n\\n\\n[마이데일리 = 구현주 기자] 서정진 셀트리온 회장이 경영복귀 ...  \n",
       "2    [\\n[방현철 박사의 머니머니] 실적, 지배구조, 건전성 우수한 코스닥 상장사는?\\...  \n",
       "3    [\\n\\t\\t\\t삼성바이오에피스, 셀트리온이 24조원에 달하는 자가면역질환 치료제 ...  \n",
       "4    [\\n삼성전자, 장중 52주 신고가 썼지만 0.41% 하락코스닥지수도 0.6% 내린...  \n",
       "..                                                 ...  \n",
       "622  [\\n홀딩스 등 4사 사내이사 겸 이사회 공동의장 후보로회사측 “글로벌 불확실성 가...  \n",
       "623  [\\n2년 임기 각 회사 사내이사 겸 공동의장 선임…28일 주총서 확정 명예회장 특...  \n",
       "624  [\\n기사내용 요약경영 복귀안 이사회 의결…현 경영진 요청글로벌 경제 불확실성 가속...  \n",
       "625  [\\n이사회서 경영 복귀안 의결, 28일 주주총회 거쳐 확정글로벌 경제 불확실성 속...  \n",
       "626  [\\nADC·mRNA 등 차세대 신약 플랫폼 개발제3공장 완공되면 생산능력 25만L...  \n",
       "\n",
       "[627 rows x 3 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newsData = pd.read_csv('../뉴스 데이터/뉴스 크롤링/관련주 크롤링/셀트리온.csv')\n",
    "newsData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "newsData.set_index('date',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2023.06.26. 오후 1:41</th>\n",
       "      <td>\\n\\t\\t\\t‘셀트리온 퀸즈 마스터즈’ 대회 설해원에서 성료...우박과 폭우속 재...</td>\n",
       "      <td>[\\n \\n설해원에서 열린 2023 셀트리온 퀸즈 마스터터즈 대회 장면. 사진제공 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-06-29</th>\n",
       "      <td>서정진 셀트리온 회장 복귀 3개월 줄줄이 부정 이슈...“어쩌나 주가!”</td>\n",
       "      <td>[\\n\\n\\n\\n\\n[마이데일리 = 구현주 기자] 서정진 셀트리온 회장이 경영복귀 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-06-29</th>\n",
       "      <td>내 주식은? 새로 지정된 코스닥 우량주 TOP 50</td>\n",
       "      <td>[\\n[방현철 박사의 머니머니] 실적, 지배구조, 건전성 우수한 코스닥 상장사는?\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-06-29</th>\n",
       "      <td>`24조 휴미라` 바이오시밀러, 사보험 길목 잡아라...삼바·셀트 `사활`</td>\n",
       "      <td>[\\n\\t\\t\\t삼성바이오에피스, 셀트리온이 24조원에 달하는 자가면역질환 치료제 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-06-29</th>\n",
       "      <td>[마켓뷰] 코스피, 기관 매도에 0.55% 하락…3일 연속 내리막</td>\n",
       "      <td>[\\n삼성전자, 장중 52주 신고가 썼지만 0.41% 하락코스닥지수도 0.6% 내린...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-03-03</th>\n",
       "      <td>셀트리온 서정진 명예회장, 소방수로 경영 복귀</td>\n",
       "      <td>[\\n홀딩스 등 4사 사내이사 겸 이사회 공동의장 후보로회사측 “글로벌 불확실성 가...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-03-03</th>\n",
       "      <td>셀트리온 서정진 컴백…바이오시밀러 美진출 총력</td>\n",
       "      <td>[\\n2년 임기 각 회사 사내이사 겸 공동의장 선임…28일 주총서 확정 명예회장 특...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-03-03</th>\n",
       "      <td>서정진 셀트리온그룹 명예회장 경영 복귀…\"위기 극복\"</td>\n",
       "      <td>[\\n기사내용 요약경영 복귀안 이사회 의결…현 경영진 요청글로벌 경제 불확실성 가속...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-03-03</th>\n",
       "      <td>서정진 셀트리온그룹 명예회장, 2년만에 경영 일선 복귀</td>\n",
       "      <td>[\\n이사회서 경영 복귀안 의결, 28일 주주총회 거쳐 확정글로벌 경제 불확실성 속...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-03-02</th>\n",
       "      <td>‘신약 기지’ 완공 앞둔 셀트리온…“송도 연구센터 밸리데이션 돌입”</td>\n",
       "      <td>[\\nADC·mRNA 등 차세대 신약 플랫폼 개발제3공장 완공되면 생산능력 25만L...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>627 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                 title   \n",
       "date                                                                     \n",
       "2023.06.26. 오후 1:41  \\n\\t\\t\\t‘셀트리온 퀸즈 마스터즈’ 대회 설해원에서 성료...우박과 폭우속 재...  \\\n",
       "2023-06-29                    서정진 셀트리온 회장 복귀 3개월 줄줄이 부정 이슈...“어쩌나 주가!”   \n",
       "2023-06-29                                내 주식은? 새로 지정된 코스닥 우량주 TOP 50   \n",
       "2023-06-29                   `24조 휴미라` 바이오시밀러, 사보험 길목 잡아라...삼바·셀트 `사활`   \n",
       "2023-06-29                        [마켓뷰] 코스피, 기관 매도에 0.55% 하락…3일 연속 내리막   \n",
       "...                                                                ...   \n",
       "2023-03-03                                   셀트리온 서정진 명예회장, 소방수로 경영 복귀   \n",
       "2023-03-03                                   셀트리온 서정진 컴백…바이오시밀러 美진출 총력   \n",
       "2023-03-03                               서정진 셀트리온그룹 명예회장 경영 복귀…\"위기 극복\"   \n",
       "2023-03-03                              서정진 셀트리온그룹 명예회장, 2년만에 경영 일선 복귀   \n",
       "2023-03-02                       ‘신약 기지’ 완공 앞둔 셀트리온…“송도 연구센터 밸리데이션 돌입”   \n",
       "\n",
       "                                                               content  \n",
       "date                                                                    \n",
       "2023.06.26. 오후 1:41  [\\n \\n설해원에서 열린 2023 셀트리온 퀸즈 마스터터즈 대회 장면. 사진제공 ...  \n",
       "2023-06-29           [\\n\\n\\n\\n\\n[마이데일리 = 구현주 기자] 서정진 셀트리온 회장이 경영복귀 ...  \n",
       "2023-06-29           [\\n[방현철 박사의 머니머니] 실적, 지배구조, 건전성 우수한 코스닥 상장사는?\\...  \n",
       "2023-06-29           [\\n\\t\\t\\t삼성바이오에피스, 셀트리온이 24조원에 달하는 자가면역질환 치료제 ...  \n",
       "2023-06-29           [\\n삼성전자, 장중 52주 신고가 썼지만 0.41% 하락코스닥지수도 0.6% 내린...  \n",
       "...                                                                ...  \n",
       "2023-03-03           [\\n홀딩스 등 4사 사내이사 겸 이사회 공동의장 후보로회사측 “글로벌 불확실성 가...  \n",
       "2023-03-03           [\\n2년 임기 각 회사 사내이사 겸 공동의장 선임…28일 주총서 확정 명예회장 특...  \n",
       "2023-03-03           [\\n기사내용 요약경영 복귀안 이사회 의결…현 경영진 요청글로벌 경제 불확실성 가속...  \n",
       "2023-03-03           [\\n이사회서 경영 복귀안 의결, 28일 주주총회 거쳐 확정글로벌 경제 불확실성 속...  \n",
       "2023-03-02           [\\nADC·mRNA 등 차세대 신약 플랫폼 개발제3공장 완공되면 생산능력 25만L...  \n",
       "\n",
       "[627 rows x 2 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newsData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text = '실적, 지배구조, 건전성 우수한 코스닥 상장사는?\\t\\t\\t\\t\\t\\t\\t29일 오후 5시 조선일보의 경제 유튜브 채널 ‘조선일보 머니’와 조선닷컴을 통해 공개된 ‘방현철 박사의 머니머니’에선 김도형 삼성자산운용본부 ETF컨설팅본부장과 함께 ‘코스닥 우등생, 코스닥 글로벌 세그먼트 투자’라는 주제로 얘기를 나눴습니다.[코스닥 우량주 TOP 50 영상으로 확인]'\n",
    "# classifier(text)[0]['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 86.44%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 86.44%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 86.44%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 86.44%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 86.44%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 89.24%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 80.74%\n",
      "이 뉴스는 긍정적 뉴스이다. 긍정적일 확률 : 70.75%\n",
      "이 뉴스는 긍정적 뉴스이다. 긍정적일 확률 : 71.66%\n",
      "이 뉴스는 긍정적 뉴스이다. 긍정적일 확률 : 68.63%\n",
      "이 뉴스는 긍정적 뉴스이다. 긍정적일 확률 : 68.63%\n",
      "이 뉴스는 긍정적 뉴스이다. 긍정적일 확률 : 52.43%\n",
      "이 뉴스는 긍정적 뉴스이다. 긍정적일 확률 : 46.5%\n",
      "이 뉴스는 긍정적 뉴스이다. 긍정적일 확률 : 46.5%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 62.07%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 77.14%\n",
      "이 뉴스는 긍정적 뉴스이다. 긍정적일 확률 : 69.93%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 91.73%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 82.88%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 82.88%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.65%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.97%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.97%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.97%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.96%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.97%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.95%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.94%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.94%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.95%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.87%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.7%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.43%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.24%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.6%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.63%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.95%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.95%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.84%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.94%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.86%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.86%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.86%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n",
      "이 뉴스는 부정적 뉴스이다. 부정적일 확률 : 99.91%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# neutral 중립, positive 긍정, negative 부정\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m733\u001b[39m):\n\u001b[1;32m----> 3\u001b[0m     \u001b[39mif\u001b[39;00m classifier(newsData[\u001b[39m'\u001b[39;49m\u001b[39mtitle\u001b[39;49m\u001b[39m'\u001b[39;49m][\u001b[39m0\u001b[39;49m][\u001b[39m0\u001b[39;49m:i])[\u001b[39m0\u001b[39m][\u001b[39m'\u001b[39m\u001b[39mlabel\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mpositive\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m      4\u001b[0m         \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39m이 뉴스는 긍정적 뉴스이다. 긍정적일 확률 : \u001b[39m\u001b[39m'\u001b[39m\u001b[39m+\u001b[39m\u001b[39mstr\u001b[39m(\u001b[39mround\u001b[39m(classifier(newsData[\u001b[39m'\u001b[39m\u001b[39mtitle\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m0\u001b[39m][\u001b[39m0\u001b[39m:i])[\u001b[39m0\u001b[39m][\u001b[39m'\u001b[39m\u001b[39mscore\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m*\u001b[39m \u001b[39m100\u001b[39m, \u001b[39m2\u001b[39m))\u001b[39m+\u001b[39m\u001b[39m'\u001b[39m\u001b[39m%\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m      5\u001b[0m     \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\PC\\anaconda3\\envs\\pandas-dev\\lib\\site-packages\\transformers\\pipelines\\text_classification.py:155\u001b[0m, in \u001b[0;36mTextClassificationPipeline.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    121\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m    122\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[39m    Classify the text(s) given as inputs.\u001b[39;00m\n\u001b[0;32m    124\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    153\u001b[0m \u001b[39m        If `top_k` is used, one such dictionary is returned per label.\u001b[39;00m\n\u001b[0;32m    154\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 155\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__call__\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    156\u001b[0m     \u001b[39m# TODO try and retrieve it in a nicer way from _sanitize_parameters.\u001b[39;00m\n\u001b[0;32m    157\u001b[0m     _legacy \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mtop_k\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m kwargs\n",
      "File \u001b[1;32mc:\\Users\\PC\\anaconda3\\envs\\pandas-dev\\lib\\site-packages\\transformers\\pipelines\\base.py:1120\u001b[0m, in \u001b[0;36mPipeline.__call__\u001b[1;34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1112\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mnext\u001b[39m(\n\u001b[0;32m   1113\u001b[0m         \u001b[39miter\u001b[39m(\n\u001b[0;32m   1114\u001b[0m             \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_iterator(\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1117\u001b[0m         )\n\u001b[0;32m   1118\u001b[0m     )\n\u001b[0;32m   1119\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1120\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrun_single(inputs, preprocess_params, forward_params, postprocess_params)\n",
      "File \u001b[1;32mc:\\Users\\PC\\anaconda3\\envs\\pandas-dev\\lib\\site-packages\\transformers\\pipelines\\base.py:1127\u001b[0m, in \u001b[0;36mPipeline.run_single\u001b[1;34m(self, inputs, preprocess_params, forward_params, postprocess_params)\u001b[0m\n\u001b[0;32m   1125\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrun_single\u001b[39m(\u001b[39mself\u001b[39m, inputs, preprocess_params, forward_params, postprocess_params):\n\u001b[0;32m   1126\u001b[0m     model_inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpreprocess(inputs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mpreprocess_params)\n\u001b[1;32m-> 1127\u001b[0m     model_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mforward(model_inputs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mforward_params)\n\u001b[0;32m   1128\u001b[0m     outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpostprocess(model_outputs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mpostprocess_params)\n\u001b[0;32m   1129\u001b[0m     \u001b[39mreturn\u001b[39;00m outputs\n",
      "File \u001b[1;32mc:\\Users\\PC\\anaconda3\\envs\\pandas-dev\\lib\\site-packages\\transformers\\pipelines\\base.py:1026\u001b[0m, in \u001b[0;36mPipeline.forward\u001b[1;34m(self, model_inputs, **forward_params)\u001b[0m\n\u001b[0;32m   1024\u001b[0m     \u001b[39mwith\u001b[39;00m inference_context():\n\u001b[0;32m   1025\u001b[0m         model_inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_ensure_tensor_on_device(model_inputs, device\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice)\n\u001b[1;32m-> 1026\u001b[0m         model_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward(model_inputs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mforward_params)\n\u001b[0;32m   1027\u001b[0m         model_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_ensure_tensor_on_device(model_outputs, device\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mdevice(\u001b[39m\"\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m\"\u001b[39m))\n\u001b[0;32m   1028\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\PC\\anaconda3\\envs\\pandas-dev\\lib\\site-packages\\transformers\\pipelines\\text_classification.py:182\u001b[0m, in \u001b[0;36mTextClassificationPipeline._forward\u001b[1;34m(self, model_inputs)\u001b[0m\n\u001b[0;32m    181\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_forward\u001b[39m(\u001b[39mself\u001b[39m, model_inputs):\n\u001b[1;32m--> 182\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_inputs)\n",
      "File \u001b[1;32mc:\\Users\\PC\\anaconda3\\envs\\pandas-dev\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\PC\\anaconda3\\envs\\pandas-dev\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:1562\u001b[0m, in \u001b[0;36mBertForSequenceClassification.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1554\u001b[0m \u001b[39m\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   1555\u001b[0m \u001b[39mlabels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\u001b[39;00m\n\u001b[0;32m   1556\u001b[0m \u001b[39m    Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\u001b[39;00m\n\u001b[0;32m   1557\u001b[0m \u001b[39m    config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[39m    `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   1560\u001b[0m return_dict \u001b[39m=\u001b[39m return_dict \u001b[39mif\u001b[39;00m return_dict \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39muse_return_dict\n\u001b[1;32m-> 1562\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbert(\n\u001b[0;32m   1563\u001b[0m     input_ids,\n\u001b[0;32m   1564\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[0;32m   1565\u001b[0m     token_type_ids\u001b[39m=\u001b[39;49mtoken_type_ids,\n\u001b[0;32m   1566\u001b[0m     position_ids\u001b[39m=\u001b[39;49mposition_ids,\n\u001b[0;32m   1567\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[0;32m   1568\u001b[0m     inputs_embeds\u001b[39m=\u001b[39;49minputs_embeds,\n\u001b[0;32m   1569\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[0;32m   1570\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[0;32m   1571\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[0;32m   1572\u001b[0m )\n\u001b[0;32m   1574\u001b[0m pooled_output \u001b[39m=\u001b[39m outputs[\u001b[39m1\u001b[39m]\n\u001b[0;32m   1576\u001b[0m pooled_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout(pooled_output)\n",
      "File \u001b[1;32mc:\\Users\\PC\\anaconda3\\envs\\pandas-dev\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\PC\\anaconda3\\envs\\pandas-dev\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:1020\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1011\u001b[0m head_mask \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_head_mask(head_mask, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mnum_hidden_layers)\n\u001b[0;32m   1013\u001b[0m embedding_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membeddings(\n\u001b[0;32m   1014\u001b[0m     input_ids\u001b[39m=\u001b[39minput_ids,\n\u001b[0;32m   1015\u001b[0m     position_ids\u001b[39m=\u001b[39mposition_ids,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1018\u001b[0m     past_key_values_length\u001b[39m=\u001b[39mpast_key_values_length,\n\u001b[0;32m   1019\u001b[0m )\n\u001b[1;32m-> 1020\u001b[0m encoder_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoder(\n\u001b[0;32m   1021\u001b[0m     embedding_output,\n\u001b[0;32m   1022\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mextended_attention_mask,\n\u001b[0;32m   1023\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[0;32m   1024\u001b[0m     encoder_hidden_states\u001b[39m=\u001b[39;49mencoder_hidden_states,\n\u001b[0;32m   1025\u001b[0m     encoder_attention_mask\u001b[39m=\u001b[39;49mencoder_extended_attention_mask,\n\u001b[0;32m   1026\u001b[0m     past_key_values\u001b[39m=\u001b[39;49mpast_key_values,\n\u001b[0;32m   1027\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[0;32m   1028\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[0;32m   1029\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[0;32m   1030\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[0;32m   1031\u001b[0m )\n\u001b[0;32m   1032\u001b[0m sequence_output \u001b[39m=\u001b[39m encoder_outputs[\u001b[39m0\u001b[39m]\n\u001b[0;32m   1033\u001b[0m pooled_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpooler(sequence_output) \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpooler \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\PC\\anaconda3\\envs\\pandas-dev\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\PC\\anaconda3\\envs\\pandas-dev\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:610\u001b[0m, in \u001b[0;36mBertEncoder.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    601\u001b[0m     layer_outputs \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mcheckpoint\u001b[39m.\u001b[39mcheckpoint(\n\u001b[0;32m    602\u001b[0m         create_custom_forward(layer_module),\n\u001b[0;32m    603\u001b[0m         hidden_states,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    607\u001b[0m         encoder_attention_mask,\n\u001b[0;32m    608\u001b[0m     )\n\u001b[0;32m    609\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 610\u001b[0m     layer_outputs \u001b[39m=\u001b[39m layer_module(\n\u001b[0;32m    611\u001b[0m         hidden_states,\n\u001b[0;32m    612\u001b[0m         attention_mask,\n\u001b[0;32m    613\u001b[0m         layer_head_mask,\n\u001b[0;32m    614\u001b[0m         encoder_hidden_states,\n\u001b[0;32m    615\u001b[0m         encoder_attention_mask,\n\u001b[0;32m    616\u001b[0m         past_key_value,\n\u001b[0;32m    617\u001b[0m         output_attentions,\n\u001b[0;32m    618\u001b[0m     )\n\u001b[0;32m    620\u001b[0m hidden_states \u001b[39m=\u001b[39m layer_outputs[\u001b[39m0\u001b[39m]\n\u001b[0;32m    621\u001b[0m \u001b[39mif\u001b[39;00m use_cache:\n",
      "File \u001b[1;32mc:\\Users\\PC\\anaconda3\\envs\\pandas-dev\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\PC\\anaconda3\\envs\\pandas-dev\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:537\u001b[0m, in \u001b[0;36mBertLayer.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    534\u001b[0m     cross_attn_present_key_value \u001b[39m=\u001b[39m cross_attention_outputs[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n\u001b[0;32m    535\u001b[0m     present_key_value \u001b[39m=\u001b[39m present_key_value \u001b[39m+\u001b[39m cross_attn_present_key_value\n\u001b[1;32m--> 537\u001b[0m layer_output \u001b[39m=\u001b[39m apply_chunking_to_forward(\n\u001b[0;32m    538\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfeed_forward_chunk, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mchunk_size_feed_forward, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mseq_len_dim, attention_output\n\u001b[0;32m    539\u001b[0m )\n\u001b[0;32m    540\u001b[0m outputs \u001b[39m=\u001b[39m (layer_output,) \u001b[39m+\u001b[39m outputs\n\u001b[0;32m    542\u001b[0m \u001b[39m# if decoder, return the attn key/values as the last output\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\PC\\anaconda3\\envs\\pandas-dev\\lib\\site-packages\\transformers\\pytorch_utils.py:237\u001b[0m, in \u001b[0;36mapply_chunking_to_forward\u001b[1;34m(forward_fn, chunk_size, chunk_dim, *input_tensors)\u001b[0m\n\u001b[0;32m    234\u001b[0m     \u001b[39m# concatenate output at same dimension\u001b[39;00m\n\u001b[0;32m    235\u001b[0m     \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39mcat(output_chunks, dim\u001b[39m=\u001b[39mchunk_dim)\n\u001b[1;32m--> 237\u001b[0m \u001b[39mreturn\u001b[39;00m forward_fn(\u001b[39m*\u001b[39;49minput_tensors)\n",
      "File \u001b[1;32mc:\\Users\\PC\\anaconda3\\envs\\pandas-dev\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:550\u001b[0m, in \u001b[0;36mBertLayer.feed_forward_chunk\u001b[1;34m(self, attention_output)\u001b[0m\n\u001b[0;32m    548\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfeed_forward_chunk\u001b[39m(\u001b[39mself\u001b[39m, attention_output):\n\u001b[0;32m    549\u001b[0m     intermediate_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mintermediate(attention_output)\n\u001b[1;32m--> 550\u001b[0m     layer_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moutput(intermediate_output, attention_output)\n\u001b[0;32m    551\u001b[0m     \u001b[39mreturn\u001b[39;00m layer_output\n",
      "File \u001b[1;32mc:\\Users\\PC\\anaconda3\\envs\\pandas-dev\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\PC\\anaconda3\\envs\\pandas-dev\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:462\u001b[0m, in \u001b[0;36mBertOutput.forward\u001b[1;34m(self, hidden_states, input_tensor)\u001b[0m\n\u001b[0;32m    461\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, hidden_states: torch\u001b[39m.\u001b[39mTensor, input_tensor: torch\u001b[39m.\u001b[39mTensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m torch\u001b[39m.\u001b[39mTensor:\n\u001b[1;32m--> 462\u001b[0m     hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdense(hidden_states)\n\u001b[0;32m    463\u001b[0m     hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout(hidden_states)\n\u001b[0;32m    464\u001b[0m     hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mLayerNorm(hidden_states \u001b[39m+\u001b[39m input_tensor)\n",
      "File \u001b[1;32mc:\\Users\\PC\\anaconda3\\envs\\pandas-dev\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\PC\\anaconda3\\envs\\pandas-dev\\lib\\site-packages\\torch\\nn\\modules\\linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# neutral 중립, positive 긍정, negative 부정\n",
    "for i in range(733):\n",
    "    if classifier(newsData['title'][0][0:i])[0]['label'] == 'positive':\n",
    "        print('이 뉴스는 긍정적 뉴스이다. 긍정적일 확률 : '+str(round(classifier(newsData['title'][0][0:i])[0]['score'] * 100, 2))+'%')\n",
    "    else:\n",
    "        print('이 뉴스는 부정적 뉴스이다. 부정적일 확률 : '+str(round(classifier(newsData['title'][0][0:i])[0]['score'] * 100, 2))+'%')\n",
    "    # else:\n",
    "    #     print('이 뉴스는 중립적 뉴스이다. 중립적일 확률 : '+str(round(classifier(newsData['title'][0][0:i])[0]['score'] * 100, 2))+'%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # neutral 중립, positive 긍정, negative 부정\n",
    "# for i in range(733):\n",
    "#     if classifier(newsData['title'][0][0:i])[0]['label'] == 'positive':\n",
    "#         print('이 뉴스는 긍정적 뉴스이다. 긍정적일 확률 : '+str(round(classifier(newsData['title'][0][0:i])[0]['score'] * 100, 2))+'%')\n",
    "#     elif classifier(newsData['title'][0][0:i])[0]['label'] == 'negative':\n",
    "#         print('이 뉴스는 부정적 뉴스이다. 부정적일 확률 : '+str(round(classifier(newsData['title'][0][0:i])[0]['score'] * 100, 2))+'%')\n",
    "#     # else:\n",
    "#     #     print('이 뉴스는 중립적 뉴스이다. 중립적일 확률 : '+str(round(classifier(newsData['title'][0][0:i])[0]['score'] * 100, 2))+'%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl-dev-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
