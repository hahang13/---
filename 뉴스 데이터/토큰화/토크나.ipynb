{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/big16/final-dev/gwajae5/하현수/뉴스 데이터/뉴스 크롤링/하이닉스'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 56\u001b[0m\n\u001b[0;32m     52\u001b[0m         DTM_DataFrmae\u001b[39m.\u001b[39mto_csv(\u001b[39m'\u001b[39m\u001b[39mD:/big16/final-dev/gwajae5/하현수/finalDTM.csv\u001b[39m\u001b[39m'\u001b[39m, encoding\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mutf-8-sig\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m     55\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m__name__\u001b[39m \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39m__main__\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m---> 56\u001b[0m     NLP_DTM()\n",
      "Cell \u001b[1;32mIn[9], line 11\u001b[0m, in \u001b[0;36mNLP_DTM\u001b[1;34m()\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mNLP_DTM\u001b[39m():\n\u001b[0;32m     10\u001b[0m     \u001b[39m# 타이틀 리스트를 불러와서 title_list 변수에 저장한다.\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m     t_file_name \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39;49m(\u001b[39m'\u001b[39;49m\u001b[39m/big16/final-dev/gwajae5/하현수/뉴스 데이터/뉴스 크롤링/하이닉스\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mr\u001b[39;49m\u001b[39m'\u001b[39;49m, encoding\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mutf-8\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[0;32m     13\u001b[0m     title_list \u001b[39m=\u001b[39m []\n\u001b[0;32m     14\u001b[0m     \u001b[39mfor\u001b[39;00m line \u001b[39min\u001b[39;00m t_file_name\u001b[39m.\u001b[39mreadlines():\n\u001b[0;32m     15\u001b[0m         \u001b[39m# txt파일을 readlines로 불러오면 개행 문자도 함께 읽어오기 때문에 인덱싱으로 처리해준다.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\PC\\anaconda3\\envs\\pandas-dev\\lib\\site-packages\\IPython\\core\\interactiveshell.py:284\u001b[0m, in \u001b[0;36m_modified_open\u001b[1;34m(file, *args, **kwargs)\u001b[0m\n\u001b[0;32m    277\u001b[0m \u001b[39mif\u001b[39;00m file \u001b[39min\u001b[39;00m {\u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m}:\n\u001b[0;32m    278\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    279\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mIPython won\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt let you open fd=\u001b[39m\u001b[39m{\u001b[39;00mfile\u001b[39m}\u001b[39;00m\u001b[39m by default \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    280\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    281\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39myou can use builtins\u001b[39m\u001b[39m'\u001b[39m\u001b[39m open.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    282\u001b[0m     )\n\u001b[1;32m--> 284\u001b[0m \u001b[39mreturn\u001b[39;00m io_open(file, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/big16/final-dev/gwajae5/하현수/뉴스 데이터/뉴스 크롤링/하이닉스'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from konlpy.tag import Okt\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "# DTM을 편리하게 만들어주기 위해 Scikit-Learn에서 제공하는 CountVectorizer를 import 한다.\n",
    "\n",
    "\n",
    "# 자연어 처리(형태소 분석 - 명사) 및 각 단어에 대한 DTM 제작\n",
    "def NLP_DTM():\n",
    "    # 타이틀 리스트를 불러와서 title_list 변수에 저장한다.\n",
    "    t_file_name = open('/big16/final-dev/gwajae5/하현수/뉴스 데이터/뉴스 크롤링/하이닉스', 'r', encoding='utf-8')\n",
    "\n",
    "    title_list = []\n",
    "    for line in t_file_name.readlines():\n",
    "        # txt파일을 readlines로 불러오면 개행 문자도 함께 읽어오기 때문에 인덱싱으로 처리해준다.\n",
    "        title_list.append(line[:-1])\n",
    "\n",
    "    t_file_name.close()\n",
    "\n",
    "    # pandas의 read_csv 함수를 이용하여 csv 파일을 불러온다.\n",
    "    dataset = pd.read_csv('/big16/final-dev/gwajae5/하현수/뉴스 데이터/뉴스 크롤링/하이닉스.csv')\n",
    "\n",
    "    # 각 형태소별로 분류(Tagging)해주는 Okt 객체를 불러온다.\n",
    "    tagger = Okt()\n",
    "\n",
    "    for title in tqdm(title_list, desc='타이틀 리스트 진행도'):  # title_list에 대해 반복문을 실행\n",
    "        # 각 타이틀에 대한 6770개 문서의 DTM을 표현하기 위해\n",
    "        # CountVectorizer 객체를 선언\n",
    "        cv = CountVectorizer()\n",
    "\n",
    "        # 각 문서들의 말뭉치(corpus)를 저장할 리스트 선언\n",
    "        corpus = []\n",
    "\n",
    "        # 각 타이틀에 대한 문서들의 말 뭉치를 저장한다. (데이터가 많으면 이 부분에서 장시간이 소요될 수 있다.)\n",
    "        for doc_num in tqdm(range(6770), desc='문서 진행도'):\n",
    "            # 각 말뭉치에서 명사 리스트를 만든다.\n",
    "            noun_list = tagger.nouns(dataset[title].loc[doc_num])\n",
    "\n",
    "            # 이를 문자열로 저장해야하기 때문에 join함수로 공백으로 구분해 corpus에 append한다.\n",
    "            corpus.append(' '.join(noun_list))\n",
    "\n",
    "        # CountVectorizer의 fit_transform 함수를 통해 DTM을 한번에 생성할 수 있다.\n",
    "        DTM_Array = cv.fit_transform(corpus).toarray()\n",
    "\n",
    "        # feature_names 함수를 사용하면 DTM의 각 열(column)이 어떤 단어에 해당하는지 알 수 있다.\n",
    "        feature_names = cv.get_feature_names()\n",
    "\n",
    "        # 추출해낸 데이터를 DataFrame 형식으로 변환한다.\n",
    "        DTM_DataFrmae = pd.DataFrame(DTM_Array, columns=feature_names)\n",
    "\n",
    "        # 최종적으로 DTM을 csv 파일로 저장한다.\n",
    "        DTM_DataFrmae.to_csv('D:/big16/final-dev/gwajae5/하현수/finalDTM.csv', encoding='utf-8-sig')\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    NLP_DTM()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>title</th>\n",
       "      <th>link</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://sports.news.naver.com/news.nhn?oid=001...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://sports.news.naver.com/news.nhn?oid=003...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://sports.news.naver.com/news.nhn?oid=001...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2023-06-25 17:21:05</td>\n",
       "      <td>외국인 짐싸나…4~5월 6조 담더니 최근 한주에 1조어치 팔았다</td>\n",
       "      <td>https://n.news.naver.com/mnews/article/009/000...</td>\n",
       "      <td>[\\n\\n\\n\\n\\n 사진은 기사 내용과 직접적인 관련이 없음. [출처 : 연합뉴스...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2023-06-25 17:20:12</td>\n",
       "      <td>초대형 유상증자에 … \"SK·CJ 투자자는 웁니다\"</td>\n",
       "      <td>https://n.news.naver.com/mnews/article/009/000...</td>\n",
       "      <td>[\\n그룹사 자금조달에 '몸살'SK이노 1.2조 유상증자하이닉스 투자부담 여전CJ는...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>877</th>\n",
       "      <td>2022-10-05 16:05:02</td>\n",
       "      <td>SK하이닉스, 반도체 노광 공정에 국산 네온가스 사용 늘린다</td>\n",
       "      <td>https://n.news.naver.com/mnews/article/028/000...</td>\n",
       "      <td>[\\n40%까지 확대…2024년엔 100% 도입\\n\\n\\n\\n경기도 이천의 SK하이...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>878</th>\n",
       "      <td>2022-10-05 15:07:01</td>\n",
       "      <td>손정의 방한했지만, SK하이닉스와 만날 계획 없어</td>\n",
       "      <td>https://n.news.naver.com/mnews/article/028/000...</td>\n",
       "      <td>[\\n이재용 부회장은 4일 만났지만, 성과 없는 듯\\n\\n\\n\\n손정의 소프트뱅크그...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>879</th>\n",
       "      <td>2022-09-29 06:08:03</td>\n",
       "      <td>위기의 반도체, 삼성도 하이닉스도 줄줄이 이익 하락 전망</td>\n",
       "      <td>https://n.news.naver.com/mnews/article/374/000...</td>\n",
       "      <td>[\\n글로벌 경기 침체 우려가 현실로 닥치면서 삼성전자와 하이닉스의 영업이익 전망치...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>880</th>\n",
       "      <td>2022-09-21 08:00:00</td>\n",
       "      <td>충북 반도체 인재, 하이닉스는 그림의 떡?…대학들 '맞춤형 교육' 절실</td>\n",
       "      <td>https://n.news.naver.com/mnews/article/421/000...</td>\n",
       "      <td>[\\n매년 700~800명 배출, 취업 연계 저조\"기업이 원하는 실무 교육 인프라 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>881</th>\n",
       "      <td>2022-09-11 06:05:00</td>\n",
       "      <td>매서운 '반도체 한파'…삼성전자·SK하이닉스 실적 전망 '내리막'</td>\n",
       "      <td>https://n.news.naver.com/mnews/article/421/000...</td>\n",
       "      <td>[\\n삼성전자, 3분기 실적 컨센서스 20%↓…SK하이닉스 3조원대 깨져\"내년까지 ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>882 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    date                                    title   \n",
       "0                    NaN                                      NaN  \\\n",
       "1                    NaN                                      NaN   \n",
       "2                    NaN                                      NaN   \n",
       "3    2023-06-25 17:21:05      외국인 짐싸나…4~5월 6조 담더니 최근 한주에 1조어치 팔았다   \n",
       "4    2023-06-25 17:20:12             초대형 유상증자에 … \"SK·CJ 투자자는 웁니다\"   \n",
       "..                   ...                                      ...   \n",
       "877  2022-10-05 16:05:02        SK하이닉스, 반도체 노광 공정에 국산 네온가스 사용 늘린다   \n",
       "878  2022-10-05 15:07:01              손정의 방한했지만, SK하이닉스와 만날 계획 없어   \n",
       "879  2022-09-29 06:08:03          위기의 반도체, 삼성도 하이닉스도 줄줄이 이익 하락 전망   \n",
       "880  2022-09-21 08:00:00  충북 반도체 인재, 하이닉스는 그림의 떡?…대학들 '맞춤형 교육' 절실   \n",
       "881  2022-09-11 06:05:00     매서운 '반도체 한파'…삼성전자·SK하이닉스 실적 전망 '내리막'   \n",
       "\n",
       "                                                  link   \n",
       "0    https://sports.news.naver.com/news.nhn?oid=001...  \\\n",
       "1    https://sports.news.naver.com/news.nhn?oid=003...   \n",
       "2    https://sports.news.naver.com/news.nhn?oid=001...   \n",
       "3    https://n.news.naver.com/mnews/article/009/000...   \n",
       "4    https://n.news.naver.com/mnews/article/009/000...   \n",
       "..                                                 ...   \n",
       "877  https://n.news.naver.com/mnews/article/028/000...   \n",
       "878  https://n.news.naver.com/mnews/article/028/000...   \n",
       "879  https://n.news.naver.com/mnews/article/374/000...   \n",
       "880  https://n.news.naver.com/mnews/article/421/000...   \n",
       "881  https://n.news.naver.com/mnews/article/421/000...   \n",
       "\n",
       "                                               content  \n",
       "0                                                   []  \n",
       "1                                                   []  \n",
       "2                                                   []  \n",
       "3    [\\n\\n\\n\\n\\n 사진은 기사 내용과 직접적인 관련이 없음. [출처 : 연합뉴스...  \n",
       "4    [\\n그룹사 자금조달에 '몸살'SK이노 1.2조 유상증자하이닉스 투자부담 여전CJ는...  \n",
       "..                                                 ...  \n",
       "877  [\\n40%까지 확대…2024년엔 100% 도입\\n\\n\\n\\n경기도 이천의 SK하이...  \n",
       "878  [\\n이재용 부회장은 4일 만났지만, 성과 없는 듯\\n\\n\\n\\n손정의 소프트뱅크그...  \n",
       "879  [\\n글로벌 경기 침체 우려가 현실로 닥치면서 삼성전자와 하이닉스의 영업이익 전망치...  \n",
       "880  [\\n매년 700~800명 배출, 취업 연계 저조\"기업이 원하는 실무 교육 인프라 ...  \n",
       "881  [\\n삼성전자, 3분기 실적 컨센서스 20%↓…SK하이닉스 3조원대 깨져\"내년까지 ...  \n",
       "\n",
       "[882 rows x 4 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('../뉴스 크롤링/하이닉스.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "ParserError",
     "evalue": "Error tokenizing data. C error: Expected 1 fields in line 26, saw 3\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mParserError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 11\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mcsv\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[39m#call data\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m train_data \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mread_csv(\u001b[39m'\u001b[39;49m\u001b[39m../뉴스 크롤링/하이닉스2.csv\u001b[39;49m\u001b[39m'\u001b[39;49m,header\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m,delimiter\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m\\t\u001b[39;49;00m\u001b[39m'\u001b[39;49m,quoting\u001b[39m=\u001b[39;49m\u001b[39m3\u001b[39;49m)\n\u001b[0;32m     12\u001b[0m test_data \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mread_csv(\u001b[39m'\u001b[39m\u001b[39m../뉴스 크롤링/하이닉스2.csv\u001b[39m\u001b[39m'\u001b[39m,header\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n\u001b[0;32m     14\u001b[0m \u001b[39m#데이터 전처리 함수\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\PC\\anaconda3\\envs\\pandas-dev\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:912\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m    899\u001b[0m kwds_defaults \u001b[39m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m    900\u001b[0m     dialect,\n\u001b[0;32m    901\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    908\u001b[0m     dtype_backend\u001b[39m=\u001b[39mdtype_backend,\n\u001b[0;32m    909\u001b[0m )\n\u001b[0;32m    910\u001b[0m kwds\u001b[39m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m--> 912\u001b[0m \u001b[39mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[1;32mc:\\Users\\PC\\anaconda3\\envs\\pandas-dev\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:583\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    580\u001b[0m     \u001b[39mreturn\u001b[39;00m parser\n\u001b[0;32m    582\u001b[0m \u001b[39mwith\u001b[39;00m parser:\n\u001b[1;32m--> 583\u001b[0m     \u001b[39mreturn\u001b[39;00m parser\u001b[39m.\u001b[39;49mread(nrows)\n",
      "File \u001b[1;32mc:\\Users\\PC\\anaconda3\\envs\\pandas-dev\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1704\u001b[0m, in \u001b[0;36mTextFileReader.read\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m   1697\u001b[0m nrows \u001b[39m=\u001b[39m validate_integer(\u001b[39m\"\u001b[39m\u001b[39mnrows\u001b[39m\u001b[39m\"\u001b[39m, nrows)\n\u001b[0;32m   1698\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1699\u001b[0m     \u001b[39m# error: \"ParserBase\" has no attribute \"read\"\u001b[39;00m\n\u001b[0;32m   1700\u001b[0m     (\n\u001b[0;32m   1701\u001b[0m         index,\n\u001b[0;32m   1702\u001b[0m         columns,\n\u001b[0;32m   1703\u001b[0m         col_dict,\n\u001b[1;32m-> 1704\u001b[0m     ) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_engine\u001b[39m.\u001b[39;49mread(  \u001b[39m# type: ignore[attr-defined]\u001b[39;49;00m\n\u001b[0;32m   1705\u001b[0m         nrows\n\u001b[0;32m   1706\u001b[0m     )\n\u001b[0;32m   1707\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m:\n\u001b[0;32m   1708\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclose()\n",
      "File \u001b[1;32mc:\\Users\\PC\\anaconda3\\envs\\pandas-dev\\lib\\site-packages\\pandas\\io\\parsers\\c_parser_wrapper.py:234\u001b[0m, in \u001b[0;36mCParserWrapper.read\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m    232\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    233\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlow_memory:\n\u001b[1;32m--> 234\u001b[0m         chunks \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_reader\u001b[39m.\u001b[39;49mread_low_memory(nrows)\n\u001b[0;32m    235\u001b[0m         \u001b[39m# destructive to chunks\u001b[39;00m\n\u001b[0;32m    236\u001b[0m         data \u001b[39m=\u001b[39m _concatenate_chunks(chunks)\n",
      "File \u001b[1;32mc:\\Users\\PC\\anaconda3\\envs\\pandas-dev\\lib\\site-packages\\pandas\\_libs\\parsers.pyx:812\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\PC\\anaconda3\\envs\\pandas-dev\\lib\\site-packages\\pandas\\_libs\\parsers.pyx:873\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\PC\\anaconda3\\envs\\pandas-dev\\lib\\site-packages\\pandas\\_libs\\parsers.pyx:848\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\PC\\anaconda3\\envs\\pandas-dev\\lib\\site-packages\\pandas\\_libs\\parsers.pyx:859\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._check_tokenize_status\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\PC\\anaconda3\\envs\\pandas-dev\\lib\\site-packages\\pandas\\_libs\\parsers.pyx:2025\u001b[0m, in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mParserError\u001b[0m: Error tokenizing data. C error: Expected 1 fields in line 26, saw 3\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from konlpy.tag import Okt\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "import csv\n",
    "\n",
    "\n",
    "#call data\n",
    "train_data = pd.read_csv('../뉴스 크롤링/하이닉스2.csv',header=0,delimiter='\\t',quoting=3)\n",
    "test_data = pd.read_csv('../뉴스 크롤링/하이닉스2.csv',header=0)\n",
    "\n",
    "#데이터 전처리 함수\n",
    "def preprocessing(review,fire_dragon=[]):\n",
    "    #한글 이외의 것 제거\n",
    "    review_text=re.sub(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\",\"\",review)\n",
    "    #Okt를 사용해 형태소 단위로 쪼개주기\n",
    "    okt=Okt()\n",
    "    word_review = okt.morphs(review_text,stem=True)\n",
    "    #불용어 제거하기\n",
    "    fire_word = [i for i in word_review if not i in fire_dragon]\n",
    "\n",
    "    return fire_word  #가볍게 전처리를 한 문자열 반환\n",
    "\n",
    "fire_dragon = ['의','이','있','하','들','그','되','수','보','않','없','나','사람','아','등','같','오','있','한'] #불용어사전\n",
    "clean_train_reviews = [] #train data 전처리한거\n",
    "clean_test_reviews = [] #test data 전처리 한거\n",
    "\n",
    "#중복 데이터 제거\n",
    "train_data.drop_duplicates(subset=['document'],inplace=True) \n",
    "test_data.drop_duplicates(subset=['document'],inplace=True) \n",
    "\n",
    "#데이터 전처리\n",
    "for review in train_data['document']:\n",
    "    if type(review) == str:\n",
    "        clean_train_reviews.append(preprocessing(review,fire_dragon=fire_dragon))\n",
    "    else:\n",
    "        clean_train_reviews.append([])\n",
    "\n",
    "for review in test_data['document']:\n",
    "    if type(review) == str:\n",
    "        clean_test_reviews.append(preprocessing(review,fire_dragon=fire_dragon))\n",
    "    else:\n",
    "        clean_test_reviews.append([])\n",
    "\n",
    "#전처리 데이터 파일로 저장 (시간절약)\n",
    "with open('clean_train_reviews.csv','w',newline='') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(clean_train_reviews)\n",
    "    writer.writerow(list(train_data['label']))\n",
    "\n",
    "with open('clean_test_reviews.csv','w',newline='') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(clean_test_reviews)\n",
    "    writer.writerow(list(test_data['label']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pandas-dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
